{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cdd48e-34bf-466e-8be3-f690943295c9",
   "metadata": {},
   "source": [
    "# Intro to 3D CNNs\n",
    "A description and demo notebook to go through creating a 3D CNN and using it with dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a29411-4500-4799-8287-e11a0434a4a6",
   "metadata": {},
   "source": [
    "## 1. Setting up dummy data\n",
    "\n",
    "Before learning how to use the cores, let's create a dummy data images. This data will be similar to a batch of images.\n",
    "\n",
    "Throughout the notebook we will refer to the elements of this shape in the following manner:\n",
    "\n",
    "[1] is the number of channels (can be input, hidden, output)\n",
    "\n",
    "[144] is the height of image or feature maps\n",
    "\n",
    "[256] is the height of image or feature maps\n",
    "\n",
    "[32] is the batch size, which is not as relevant for understanding the material in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fbd21f-f5d6-4967-be9d-00112297613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access to neuropixel_predictor\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# Essential Imports\n",
    "import torch\n",
    "\n",
    "\n",
    "images = torch.ones(32, 1, 144, 256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d38b27-e824-4d21-aafb-f5dfba941ae2",
   "metadata": {},
   "source": [
    "## 2. Using Stacked 3D Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f69f869-558b-491e-badd-c71f3600c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked3dcore_config = {\n",
    "    # core args\n",
    "    'input_channels': 1,\n",
    "    'input_kernel': 9,\n",
    "    'hidden_kernel': 7,\n",
    "    'hidden_channels': 64,\n",
    "    'layers': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "172fcacf-3cba-4bac-b996-f4ca81c7558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarek/Documents/Repos/neuropixel-predictor/notebooks/../neuropixel_predictor/layers/cores/base.py:82: UserWarning: The batch_norm is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/Users/tarek/Documents/Repos/neuropixel-predictor/notebooks/../neuropixel_predictor/layers/cores/base.py:82: UserWarning: The hidden_channels is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/Users/tarek/Documents/Repos/neuropixel-predictor/notebooks/../neuropixel_predictor/layers/cores/base.py:82: UserWarning: The bias is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/Users/tarek/Documents/Repos/neuropixel-predictor/notebooks/../neuropixel_predictor/layers/cores/base.py:82: UserWarning: The batch_norm_scale is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for %: 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneuropixel_predictor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Basic3dCore \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m stacked3d_core = \u001b[43mBasic3dCore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mstacked3dcore_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m stacked3dcore_out = stacked3d_core(images)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(stacked3dcore_out.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Repos/neuropixel-predictor/notebooks/../neuropixel_predictor/layers/cores/conv3d.py:175\u001b[39m, in \u001b[36mBasic3dCore.__init__\u001b[39m\u001b[34m(self, input_channels, hidden_channels, input_kernel, hidden_kernel, layers, stride, gamma_input_spatial, gamma_input_temporal, hidden_nonlinearities, x_shift, y_shift, bias, batch_norm, padding, batch_norm_scale, momentum, laplace_padding, input_regularizer, cuda, final_nonlin, spatial_dilation, temporal_dilation, hidden_spatial_dilation, hidden_temporal_dilation)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.layers - \u001b[32m1\u001b[39m):\n\u001b[32m    174\u001b[39m     layer = OrderedDict()\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     layer[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconv_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_temporal_dilation\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_spatial_dilation\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_spatial_dilation\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_bn_layer(layer=layer, layer_idx=l + \u001b[32m1\u001b[39m)\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.final_nonlinearity \u001b[38;5;129;01mor\u001b[39;00m l < \u001b[38;5;28mself\u001b[39m.layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/np-predictor/lib/python3.13/site-packages/torch/nn/modules/conv.py:692\u001b[39m, in \u001b[36mConv3d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    690\u001b[39m padding_ = padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _triple(padding)\n\u001b[32m    691\u001b[39m dilation_ = _triple(dilation)\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_triple\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/np-predictor/lib/python3.13/site-packages/torch/nn/modules/conv.py:106\u001b[39m, in \u001b[36m_ConvNd.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m groups <= \u001b[32m0\u001b[39m:\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgroups must be a positive integer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43min_channels\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m != \u001b[32m0\u001b[39m:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33min_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_channels % groups != \u001b[32m0\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for %: 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "from neuropixel_predictor.layers.cores import Basic3dCore \n",
    "\n",
    "stacked3d_core = Basic3dCore(**stacked3dcore_config)\n",
    "stacked3dcore_out = stacked3d_core(images)\n",
    "print(stacked3dcore_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5d3b8-9771-428a-93c9-bb0f0b3232e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
